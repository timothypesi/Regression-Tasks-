{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Linear regression tips.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit ('base': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "8e24f623c9d976e65e43b538ecbbc4d478524c94015e92b14b460358aba5245a"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Example: Detecting multicollinearity\n",
        "\n",
        "We will once again use our university admission [dataset](https://drive.google.com/file/d/13HPgfc4HP9UP-gHM2lutJ7MXSt-dAL1d/view)\n",
        "\n",
        "First, let's understand how our independent variables are correlated to each other"
      ],
      "metadata": {
        "id": "oxXI2w6xSCxt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "# Load the dataset\r\n",
        "data = pd.read_csv('data/uni_admission.csv')\r\n",
        "\r\n",
        "# Remove the serial and admit chance columns, we want to focus on our independent variables only.\r\n",
        "independent_only = data.drop(columns=['Serial No.', 'admit_chance'])\r\n",
        "\r\n",
        "# Let's display the correlations between the variables\r\n",
        "correlations = independent_only.corr()\r\n",
        "\r\n",
        "correlations"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>GRE</th>\n",
              "      <th>TOEFL</th>\n",
              "      <th>uni_rating</th>\n",
              "      <th>SOP</th>\n",
              "      <th>LOR</th>\n",
              "      <th>CGPA</th>\n",
              "      <th>publications</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>GRE</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.827200</td>\n",
              "      <td>0.635376</td>\n",
              "      <td>0.613498</td>\n",
              "      <td>0.524679</td>\n",
              "      <td>0.825878</td>\n",
              "      <td>0.563398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TOEFL</th>\n",
              "      <td>0.827200</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.649799</td>\n",
              "      <td>0.644410</td>\n",
              "      <td>0.541563</td>\n",
              "      <td>0.810574</td>\n",
              "      <td>0.467012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>uni_rating</th>\n",
              "      <td>0.635376</td>\n",
              "      <td>0.649799</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.728024</td>\n",
              "      <td>0.608651</td>\n",
              "      <td>0.705254</td>\n",
              "      <td>0.427047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SOP</th>\n",
              "      <td>0.613498</td>\n",
              "      <td>0.644410</td>\n",
              "      <td>0.728024</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.663707</td>\n",
              "      <td>0.712154</td>\n",
              "      <td>0.408116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LOR</th>\n",
              "      <td>0.524679</td>\n",
              "      <td>0.541563</td>\n",
              "      <td>0.608651</td>\n",
              "      <td>0.663707</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.637469</td>\n",
              "      <td>0.372526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CGPA</th>\n",
              "      <td>0.825878</td>\n",
              "      <td>0.810574</td>\n",
              "      <td>0.705254</td>\n",
              "      <td>0.712154</td>\n",
              "      <td>0.637469</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.501311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>publications</th>\n",
              "      <td>0.563398</td>\n",
              "      <td>0.467012</td>\n",
              "      <td>0.427047</td>\n",
              "      <td>0.408116</td>\n",
              "      <td>0.372526</td>\n",
              "      <td>0.501311</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   GRE     TOEFL  uni_rating       SOP       LOR      CGPA  \\\n",
              "GRE           1.000000  0.827200    0.635376  0.613498  0.524679  0.825878   \n",
              "TOEFL         0.827200  1.000000    0.649799  0.644410  0.541563  0.810574   \n",
              "uni_rating    0.635376  0.649799    1.000000  0.728024  0.608651  0.705254   \n",
              "SOP           0.613498  0.644410    0.728024  1.000000  0.663707  0.712154   \n",
              "LOR           0.524679  0.541563    0.608651  0.663707  1.000000  0.637469   \n",
              "CGPA          0.825878  0.810574    0.705254  0.712154  0.637469  1.000000   \n",
              "publications  0.563398  0.467012    0.427047  0.408116  0.372526  0.501311   \n",
              "\n",
              "              publications  \n",
              "GRE               0.563398  \n",
              "TOEFL             0.467012  \n",
              "uni_rating        0.427047  \n",
              "SOP               0.408116  \n",
              "LOR               0.372526  \n",
              "CGPA              0.501311  \n",
              "publications      1.000000  "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "metadata": {
        "id": "3myU4PcKSB9V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "e9e4a86f-0d5c-4904-c849-4e5f974f8dc3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "source": [
        "data.describe()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Serial No.</th>\n",
              "      <th>GRE</th>\n",
              "      <th>TOEFL</th>\n",
              "      <th>uni_rating</th>\n",
              "      <th>SOP</th>\n",
              "      <th>LOR</th>\n",
              "      <th>CGPA</th>\n",
              "      <th>publications</th>\n",
              "      <th>admit_chance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>500.000000</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>500.00000</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>500.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>250.500000</td>\n",
              "      <td>316.472000</td>\n",
              "      <td>107.192000</td>\n",
              "      <td>3.114000</td>\n",
              "      <td>3.374000</td>\n",
              "      <td>3.48400</td>\n",
              "      <td>8.576440</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.72174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>144.481833</td>\n",
              "      <td>11.295148</td>\n",
              "      <td>6.081868</td>\n",
              "      <td>1.143512</td>\n",
              "      <td>0.991004</td>\n",
              "      <td>0.92545</td>\n",
              "      <td>0.604813</td>\n",
              "      <td>0.496884</td>\n",
              "      <td>0.14114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>290.000000</td>\n",
              "      <td>92.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>6.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.34000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>125.750000</td>\n",
              "      <td>308.000000</td>\n",
              "      <td>103.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>3.00000</td>\n",
              "      <td>8.127500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.63000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>250.500000</td>\n",
              "      <td>317.000000</td>\n",
              "      <td>107.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.500000</td>\n",
              "      <td>3.50000</td>\n",
              "      <td>8.560000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.72000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>375.250000</td>\n",
              "      <td>325.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.00000</td>\n",
              "      <td>9.040000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.82000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>500.000000</td>\n",
              "      <td>340.000000</td>\n",
              "      <td>120.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.00000</td>\n",
              "      <td>9.920000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.97000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Serial No.         GRE       TOEFL  uni_rating         SOP        LOR  \\\n",
              "count  500.000000  500.000000  500.000000  500.000000  500.000000  500.00000   \n",
              "mean   250.500000  316.472000  107.192000    3.114000    3.374000    3.48400   \n",
              "std    144.481833   11.295148    6.081868    1.143512    0.991004    0.92545   \n",
              "min      1.000000  290.000000   92.000000    1.000000    1.000000    1.00000   \n",
              "25%    125.750000  308.000000  103.000000    2.000000    2.500000    3.00000   \n",
              "50%    250.500000  317.000000  107.000000    3.000000    3.500000    3.50000   \n",
              "75%    375.250000  325.000000  112.000000    4.000000    4.000000    4.00000   \n",
              "max    500.000000  340.000000  120.000000    5.000000    5.000000    5.00000   \n",
              "\n",
              "             CGPA  publications  admit_chance  \n",
              "count  500.000000    500.000000     500.00000  \n",
              "mean     8.576440      0.560000       0.72174  \n",
              "std      0.604813      0.496884       0.14114  \n",
              "min      6.800000      0.000000       0.34000  \n",
              "25%      8.127500      0.000000       0.63000  \n",
              "50%      8.560000      1.000000       0.72000  \n",
              "75%      9.040000      1.000000       0.82000  \n",
              "max      9.920000      1.000000       0.97000  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tabe above shows us how each variable relates to another. The coefficient of 1 across the diagonal makes sense, as a variable is perfectly correlated to itself. Let's use these correlations to compute the VIF score for each variable. This will require a little bit of linear algebra, but the approach is straightforward: we create a new dataframe with the *inverse* of the matrix above."
      ],
      "metadata": {
        "id": "l3mXiNPXcMeF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "pd.DataFrame(np.linalg.inv(correlations.values), index = correlations.index, columns=correlations.columns)\r\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>GRE</th>\n",
              "      <th>TOEFL</th>\n",
              "      <th>uni_rating</th>\n",
              "      <th>SOP</th>\n",
              "      <th>LOR</th>\n",
              "      <th>CGPA</th>\n",
              "      <th>publications</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>GRE</th>\n",
              "      <td>4.464249</td>\n",
              "      <td>-1.919309</td>\n",
              "      <td>-0.167441</td>\n",
              "      <td>0.115539</td>\n",
              "      <td>0.163716</td>\n",
              "      <td>-1.829666</td>\n",
              "      <td>-0.738214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TOEFL</th>\n",
              "      <td>-1.919309</td>\n",
              "      <td>3.904213</td>\n",
              "      <td>-0.280590</td>\n",
              "      <td>-0.320530</td>\n",
              "      <td>0.008925</td>\n",
              "      <td>-1.216918</td>\n",
              "      <td>0.115389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>uni_rating</th>\n",
              "      <td>-0.167441</td>\n",
              "      <td>-0.280590</td>\n",
              "      <td>2.621036</td>\n",
              "      <td>-1.003439</td>\n",
              "      <td>-0.326820</td>\n",
              "      <td>-0.504916</td>\n",
              "      <td>-0.109544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SOP</th>\n",
              "      <td>0.115539</td>\n",
              "      <td>-0.320530</td>\n",
              "      <td>-1.003439</td>\n",
              "      <td>2.835210</td>\n",
              "      <td>-0.715324</td>\n",
              "      <td>-0.670228</td>\n",
              "      <td>-0.041512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LOR</th>\n",
              "      <td>0.163716</td>\n",
              "      <td>0.008925</td>\n",
              "      <td>-0.326820</td>\n",
              "      <td>-0.715324</td>\n",
              "      <td>2.033555</td>\n",
              "      <td>-0.650578</td>\n",
              "      <td>-0.096312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CGPA</th>\n",
              "      <td>-1.829666</td>\n",
              "      <td>-1.216918</td>\n",
              "      <td>-0.504916</td>\n",
              "      <td>-0.670228</td>\n",
              "      <td>-0.650578</td>\n",
              "      <td>4.777992</td>\n",
              "      <td>-0.064604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>publications</th>\n",
              "      <td>-0.738214</td>\n",
              "      <td>0.115389</td>\n",
              "      <td>-0.109544</td>\n",
              "      <td>-0.041512</td>\n",
              "      <td>-0.096312</td>\n",
              "      <td>-0.064604</td>\n",
              "      <td>1.494008</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   GRE     TOEFL  uni_rating       SOP       LOR      CGPA  \\\n",
              "GRE           4.464249 -1.919309   -0.167441  0.115539  0.163716 -1.829666   \n",
              "TOEFL        -1.919309  3.904213   -0.280590 -0.320530  0.008925 -1.216918   \n",
              "uni_rating   -0.167441 -0.280590    2.621036 -1.003439 -0.326820 -0.504916   \n",
              "SOP           0.115539 -0.320530   -1.003439  2.835210 -0.715324 -0.670228   \n",
              "LOR           0.163716  0.008925   -0.326820 -0.715324  2.033555 -0.650578   \n",
              "CGPA         -1.829666 -1.216918   -0.504916 -0.670228 -0.650578  4.777992   \n",
              "publications -0.738214  0.115389   -0.109544 -0.041512 -0.096312 -0.064604   \n",
              "\n",
              "              publications  \n",
              "GRE              -0.738214  \n",
              "TOEFL             0.115389  \n",
              "uni_rating       -0.109544  \n",
              "SOP              -0.041512  \n",
              "LOR              -0.096312  \n",
              "CGPA             -0.064604  \n",
              "publications      1.494008  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "metadata": {
        "id": "Q_L2Glx4UJjW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "d10406df-158c-485c-bec7-3e20b4f9724f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpreting the table is straightforward: The VIF score for each variable is found alongside the downwards sloping diagonal. GRE has a score of 4.46, TOEFL has a score of 3.9, uni_rating a score of 2.62, etc.\n",
        "\n",
        "CGPA has a value nearing 5, let's see how the VIF scores improve if we remove it from our dataset"
      ],
      "metadata": {
        "id": "jTBZHScKc5yh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "revised = independent_only.drop(columns=['CGPA','TOEFL'])\r\n",
        "\r\n",
        "correlations = revised.corr()\r\n",
        "pd.DataFrame(np.linalg.inv(correlations.values), index = correlations.index, columns=correlations.columns)\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>GRE</th>\n",
              "      <th>uni_rating</th>\n",
              "      <th>SOP</th>\n",
              "      <th>LOR</th>\n",
              "      <th>publications</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>GRE</th>\n",
              "      <td>2.180611</td>\n",
              "      <td>-0.632346</td>\n",
              "      <td>-0.467117</td>\n",
              "      <td>-0.189454</td>\n",
              "      <td>-0.697296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>uni_rating</th>\n",
              "      <td>-0.632346</td>\n",
              "      <td>2.521095</td>\n",
              "      <td>-1.130190</td>\n",
              "      <td>-0.413417</td>\n",
              "      <td>-0.105108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SOP</th>\n",
              "      <td>-0.467117</td>\n",
              "      <td>-1.130190</td>\n",
              "      <td>2.674058</td>\n",
              "      <td>-0.828010</td>\n",
              "      <td>-0.037053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LOR</th>\n",
              "      <td>-0.189454</td>\n",
              "      <td>-0.413417</td>\n",
              "      <td>-0.828010</td>\n",
              "      <td>1.938133</td>\n",
              "      <td>-0.100793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>publications</th>\n",
              "      <td>-0.697296</td>\n",
              "      <td>-0.105108</td>\n",
              "      <td>-0.037053</td>\n",
              "      <td>-0.100793</td>\n",
              "      <td>1.490411</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   GRE  uni_rating       SOP       LOR  publications\n",
              "GRE           2.180611   -0.632346 -0.467117 -0.189454     -0.697296\n",
              "uni_rating   -0.632346    2.521095 -1.130190 -0.413417     -0.105108\n",
              "SOP          -0.467117   -1.130190  2.674058 -0.828010     -0.037053\n",
              "LOR          -0.189454   -0.413417 -0.828010  1.938133     -0.100793\n",
              "publications -0.697296   -0.105108 -0.037053 -0.100793      1.490411"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "metadata": {
        "id": "8ODEkFCQaG94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "f466f621-3d40-45cc-f2d5-4a7711251640"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using VIF in other ways to determin weight of variables "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "source": [
        "#Let me do it using the VIF function\r\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\r\n",
        "X=data.drop(columns=['Serial No.', 'admit_chance'])\r\n",
        "vif = pd.DataFrame()\r\n",
        "vif[\"features\"] = X.columns\r\n",
        "vif[\"vif_Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\r\n",
        "print(vif)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       features   vif_Factor\n",
            "0           GRE  1308.061089\n",
            "1         TOEFL  1215.951898\n",
            "2    uni_rating    20.933361\n",
            "3           SOP    35.265006\n",
            "4           LOR    30.911476\n",
            "5          CGPA   950.817985\n",
            "6  publications     2.869493\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "source": [
        "cols = list(vif['features'])\r\n",
        "cols"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['GRE', 'TOEFL', 'uni_rating', 'SOP', 'LOR', 'CGPA', 'publications']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "source": [
        "data.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Serial No.</th>\n",
              "      <th>GRE</th>\n",
              "      <th>TOEFL</th>\n",
              "      <th>uni_rating</th>\n",
              "      <th>SOP</th>\n",
              "      <th>LOR</th>\n",
              "      <th>CGPA</th>\n",
              "      <th>publications</th>\n",
              "      <th>admit_chance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>337</td>\n",
              "      <td>118</td>\n",
              "      <td>4</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>9.65</td>\n",
              "      <td>1</td>\n",
              "      <td>0.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>324</td>\n",
              "      <td>107</td>\n",
              "      <td>4</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.5</td>\n",
              "      <td>8.87</td>\n",
              "      <td>1</td>\n",
              "      <td>0.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>316</td>\n",
              "      <td>104</td>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>8.00</td>\n",
              "      <td>1</td>\n",
              "      <td>0.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>322</td>\n",
              "      <td>110</td>\n",
              "      <td>3</td>\n",
              "      <td>3.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>8.67</td>\n",
              "      <td>1</td>\n",
              "      <td>0.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>314</td>\n",
              "      <td>103</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>8.21</td>\n",
              "      <td>0</td>\n",
              "      <td>0.65</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Serial No.  GRE  TOEFL  uni_rating  SOP  LOR  CGPA  publications  \\\n",
              "0           1  337    118           4  4.5  4.5  9.65             1   \n",
              "1           2  324    107           4  4.0  4.5  8.87             1   \n",
              "2           3  316    104           3  3.0  3.5  8.00             1   \n",
              "3           4  322    110           3  3.5  2.5  8.67             1   \n",
              "4           5  314    103           2  2.0  3.0  8.21             0   \n",
              "\n",
              "   admit_chance  \n",
              "0          0.92  \n",
              "1          0.76  \n",
              "2          0.72  \n",
              "3          0.80  \n",
              "4          0.65  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "source": [
        "X = data[cols]\r\n",
        "y = data['admit_chance']\r\n",
        "y"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      0.92\n",
              "1      0.76\n",
              "2      0.72\n",
              "3      0.80\n",
              "4      0.65\n",
              "       ... \n",
              "495    0.87\n",
              "496    0.96\n",
              "497    0.93\n",
              "498    0.73\n",
              "499    0.84\n",
              "Name: admit_chance, Length: 500, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "source": [
        "# Train using 80% of the data.\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.linear_model import LinearRegression\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\r\n",
        "\r\n",
        "# find optimal coefficients and intercept\r\n",
        "regressor = LinearRegression()  \r\n",
        "regressor.fit(X_train, y_train)\r\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "source": [
        "from sklearn import metrics\r\n",
        "\r\n",
        "y_pred = regressor.predict(X_test)\r\n",
        "\r\n",
        "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \r\n",
        "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \r\n",
        "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\r\n",
        "\r\n",
        "#  Gives a better RMSSE even when you dont do away with the values that have high VIF \r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error: 0.048253893748654714\n",
            "Mean Squared Error: 0.004079680034602104\n",
            "Root Mean Squared Error: 0.06387237301527245\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "All scores dropped, but the GRE's in particular did quite a bit, indicating that GRE and CGPA were colinear. "
      ],
      "metadata": {
        "id": "Z95zlmBidiF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 2: Residual plots and heteroskedasticity testing\r\n",
        "\r\n",
        "Let's start by creating a model based on our revised set of independent variables above, then displaying the residual plot for it."
      ],
      "metadata": {
        "id": "7DRwrZjZduD7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "source": [
        "from sklearn import metrics\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.linear_model import LinearRegression\r\n",
        "\r\n",
        "X = revised.values\r\n",
        "y = data['admit_chance'].values\r\n",
        "#print(X)\r\n",
        "\r\n",
        "X_train, X_test, admit_train, admit_test = train_test_split(X, y, test_size=0.2, random_state=0)\r\n",
        "\r\n",
        "regressor = LinearRegression()\r\n",
        "regressor.fit(X_train, admit_train)\r\n",
        "\r\n",
        "# This is our prediction for admission based on our model\r\n",
        "admit_predict = regressor.predict(X_test)\r\n",
        "#print(admit_predict)\r\n",
        "\r\n",
        "# We now create the residual by substracting the test value from the predicted \r\n",
        "# value for each row in our dataset\r\n",
        "\r\n",
        "residuals = np.subtract(admit_predict, admit_test)\r\n",
        "\r\n",
        "# Let's describe our residual:\r\n",
        "pd.DataFrame(residuals).describe()\r\n",
        "\r\n",
        "residuals.mean()\r\n",
        "#print(residuals)\r\n",
        "print(residuals.mean())\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.004327928908138407\n"
          ]
        }
      ],
      "metadata": {
        "id": "IHQOLs1Nh2bx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "45772409-dfe6-43f8-d545-04500fb8d13a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we interpret the description above, let's recall what we are trying to predict: The percentage chance of admission to university. This means values between 0 and 1. \n",
        "\n",
        "Our min and max for the residual are fairly high: they suggest we've been up to 26% off target. It's important for us to plot this first: Is this a common occurence, or a few outliers?\n",
        "\n",
        "Our mean on the other hand is close to 0, indicating that we tend to be fairly correct, although slightly over estimating chances by, on average, 0.17%\n",
        "\n",
        "Let's show the residual plot"
      ],
      "metadata": {
        "id": "wevY_SU-j7gH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "plt.scatter(admit_predict, residuals, color='black')\r\n",
        "plt.ylabel('residual')\r\n",
        "plt.xlabel('fitted values')\r\n",
        "plt.axhline(y= residuals.mean(), color='red', linewidth=1)\r\n",
        "plt.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "metadata": {
        "id": "sn8crVSAlPIw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "d81c4196-3cf4-476d-c403-7cdeabdf975d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This does not look too bad: our residuals are centered around a mean that is very close to 0, and there are no glaringly obvious patterns. Let's be thorough though, and perform a heteroskedasticity test.\n",
        "\n",
        "For this we will use [bartlett's test](https://www.itl.nist.gov/div898/handbook/eda/section3/eda357.htm). The test establishes as a null hypothesis that the variance is equal for all our datapoints,and the new hypothesis that the variance is different for at least one pair of datapoints.\n"
      ],
      "metadata": {
        "id": "oIT6DpvlmqaD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "source": [
        "import scipy as sp\r\n",
        "\r\n",
        "test_result, p_value = sp.stats.bartlett(admit_predict, residuals)\r\n",
        "\r\n",
        "# To interpret the results we must also compute a critical value of the chi squared distribution\r\n",
        "degree_of_freedom = len(admit_predict)-1\r\n",
        "probability = 1 - p_value\r\n",
        "\r\n",
        "critical_value = sp.stats.chi2.ppf(probability, degree_of_freedom)\r\n",
        "print(critical_value )\r\n",
        "\r\n",
        "# If the test_result is greater than the critical value, then we reject our null\r\n",
        "# hypothesis. This would mean that there are patterns to the variance of the data\r\n",
        "\r\n",
        "# Otherwise, we can identify no patterns, and we accept the null hypothesis that \r\n",
        "# the variance is homogeneous across our data\r\n",
        "\r\n",
        "if (test_result > critical_value):\r\n",
        "  print('the variances are unequal, and the model should be reassessed')\r\n",
        "else:\r\n",
        "  print('the variances are homogeneous!')\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179.79090295708536\n",
            "the variances are homogeneous!\n"
          ]
        }
      ],
      "metadata": {
        "id": "LlvHQ6Ivm5j5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "ac5d7635-1ab7-43d9-a0ae-093fdb218019"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Challenge\n",
        "\n",
        "We will bring this all together by adapting a challenge from data.world. In the code section below you have access to a large dataset with demographic and medical data for cancer occurances in regions in the US. Your aim is to build a model that predicts the **target_deathrate** variable. You can find descriptions of all the columns [here](https://data.world/exercises/linear-regression-exercise-1)\n",
        "\n",
        "This is a holistic challenge:\n",
        "\n",
        "\n",
        "*   Make sure to clean up your data first, there are some missing elements.\n",
        "*   Some data should be changed: Add a **state** column which indicates the state a person lives in(you can modify the geography column). Perform a multicollinearity test: Should **state** be included in your model or not?\n",
        "* If it should be, make sure to transform it since we want to be dealing with numerical data.\n",
        "* Build a model, then display the residual plot for it. Perform a Bartlett test to determine if your model is acceptable or not.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W27Z9P7Jx1yK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "source": [
        "# Your code goes here!\r\n",
        "df = pd.read_csv('https://query.data.world/s/4n3rdrpeu6qrfo63yacah7v3ofoaag')\r\n",
        "df.info()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3047 entries, 0 to 3046\n",
            "Data columns (total 33 columns):\n",
            " #   Column                   Non-Null Count  Dtype  \n",
            "---  ------                   --------------  -----  \n",
            " 0   avganncount              3047 non-null   float64\n",
            " 1   avgdeathsperyear         3047 non-null   int64  \n",
            " 2   target_deathrate         3047 non-null   float64\n",
            " 3   incidencerate            3047 non-null   float64\n",
            " 4   medincome                3047 non-null   int64  \n",
            " 5   popest2015               3047 non-null   int64  \n",
            " 6   povertypercent           3047 non-null   float64\n",
            " 7   studypercap              3047 non-null   float64\n",
            " 8   binnedinc                3047 non-null   object \n",
            " 9   medianage                3047 non-null   float64\n",
            " 10  medianagemale            3047 non-null   float64\n",
            " 11  medianagefemale          3047 non-null   float64\n",
            " 12  geography                3047 non-null   object \n",
            " 13  percentmarried           3047 non-null   float64\n",
            " 14  pctnohs18_24             3047 non-null   float64\n",
            " 15  pcths18_24               3047 non-null   float64\n",
            " 16  pctsomecol18_24          762 non-null    float64\n",
            " 17  pctbachdeg18_24          3047 non-null   float64\n",
            " 18  pcths25_over             3047 non-null   float64\n",
            " 19  pctbachdeg25_over        3047 non-null   float64\n",
            " 20  pctemployed16_over       2895 non-null   float64\n",
            " 21  pctunemployed16_over     3047 non-null   float64\n",
            " 22  pctprivatecoverage       3047 non-null   float64\n",
            " 23  pctprivatecoveragealone  2438 non-null   float64\n",
            " 24  pctempprivcoverage       3047 non-null   float64\n",
            " 25  pctpubliccoverage        3047 non-null   float64\n",
            " 26  pctpubliccoveragealone   3047 non-null   float64\n",
            " 27  pctwhite                 3047 non-null   float64\n",
            " 28  pctblack                 3047 non-null   float64\n",
            " 29  pctasian                 3047 non-null   float64\n",
            " 30  pctotherrace             3047 non-null   float64\n",
            " 31  pctmarriedhouseholds     3047 non-null   float64\n",
            " 32  birthrate                3047 non-null   float64\n",
            "dtypes: float64(28), int64(3), object(2)\n",
            "memory usage: 785.7+ KB\n"
          ]
        }
      ],
      "metadata": {
        "id": "jPivvZE_zCzY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "source": [
        "import seaborn as sns\r\n",
        "sns.heatmap(df.isnull(), yticklabels=False,cbar=False, cmap='viridis')"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "metadata": {},
          "execution_count": 104
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "source": [
        "df.describe()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>avganncount</th>\n",
              "      <th>avgdeathsperyear</th>\n",
              "      <th>target_deathrate</th>\n",
              "      <th>incidencerate</th>\n",
              "      <th>medincome</th>\n",
              "      <th>popest2015</th>\n",
              "      <th>povertypercent</th>\n",
              "      <th>studypercap</th>\n",
              "      <th>medianage</th>\n",
              "      <th>medianagemale</th>\n",
              "      <th>...</th>\n",
              "      <th>pctprivatecoveragealone</th>\n",
              "      <th>pctempprivcoverage</th>\n",
              "      <th>pctpubliccoverage</th>\n",
              "      <th>pctpubliccoveragealone</th>\n",
              "      <th>pctwhite</th>\n",
              "      <th>pctblack</th>\n",
              "      <th>pctasian</th>\n",
              "      <th>pctotherrace</th>\n",
              "      <th>pctmarriedhouseholds</th>\n",
              "      <th>birthrate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>3047.000000</td>\n",
              "      <td>3047.000000</td>\n",
              "      <td>3047.000000</td>\n",
              "      <td>3047.000000</td>\n",
              "      <td>3047.000000</td>\n",
              "      <td>3.047000e+03</td>\n",
              "      <td>3047.000000</td>\n",
              "      <td>3047.000000</td>\n",
              "      <td>3047.000000</td>\n",
              "      <td>3047.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>3047.000000</td>\n",
              "      <td>3047.000000</td>\n",
              "      <td>3047.000000</td>\n",
              "      <td>3047.000000</td>\n",
              "      <td>3047.000000</td>\n",
              "      <td>3047.000000</td>\n",
              "      <td>3047.000000</td>\n",
              "      <td>3047.000000</td>\n",
              "      <td>3047.000000</td>\n",
              "      <td>3047.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>606.338544</td>\n",
              "      <td>185.965868</td>\n",
              "      <td>178.664063</td>\n",
              "      <td>448.268586</td>\n",
              "      <td>47063.281917</td>\n",
              "      <td>1.026374e+05</td>\n",
              "      <td>16.878175</td>\n",
              "      <td>155.399415</td>\n",
              "      <td>45.272333</td>\n",
              "      <td>39.570725</td>\n",
              "      <td>...</td>\n",
              "      <td>48.453774</td>\n",
              "      <td>41.196324</td>\n",
              "      <td>36.252642</td>\n",
              "      <td>19.240072</td>\n",
              "      <td>83.645286</td>\n",
              "      <td>9.107978</td>\n",
              "      <td>1.253965</td>\n",
              "      <td>1.983523</td>\n",
              "      <td>51.243872</td>\n",
              "      <td>5.640306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1416.356223</td>\n",
              "      <td>504.134286</td>\n",
              "      <td>27.751511</td>\n",
              "      <td>54.560733</td>\n",
              "      <td>12040.090836</td>\n",
              "      <td>3.290592e+05</td>\n",
              "      <td>6.409087</td>\n",
              "      <td>529.628366</td>\n",
              "      <td>45.304480</td>\n",
              "      <td>5.226017</td>\n",
              "      <td>...</td>\n",
              "      <td>9.018885</td>\n",
              "      <td>9.447687</td>\n",
              "      <td>7.841741</td>\n",
              "      <td>6.113041</td>\n",
              "      <td>16.380025</td>\n",
              "      <td>14.534538</td>\n",
              "      <td>2.610276</td>\n",
              "      <td>3.517710</td>\n",
              "      <td>6.572814</td>\n",
              "      <td>1.985816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>59.700000</td>\n",
              "      <td>201.300000</td>\n",
              "      <td>22640.000000</td>\n",
              "      <td>8.270000e+02</td>\n",
              "      <td>3.200000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>22.300000</td>\n",
              "      <td>22.400000</td>\n",
              "      <td>...</td>\n",
              "      <td>15.700000</td>\n",
              "      <td>13.500000</td>\n",
              "      <td>11.200000</td>\n",
              "      <td>2.600000</td>\n",
              "      <td>10.199155</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>22.992490</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>76.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>161.200000</td>\n",
              "      <td>420.300000</td>\n",
              "      <td>38882.500000</td>\n",
              "      <td>1.168400e+04</td>\n",
              "      <td>12.150000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.700000</td>\n",
              "      <td>36.350000</td>\n",
              "      <td>...</td>\n",
              "      <td>43.100000</td>\n",
              "      <td>34.500000</td>\n",
              "      <td>30.900000</td>\n",
              "      <td>14.850000</td>\n",
              "      <td>77.296180</td>\n",
              "      <td>0.620675</td>\n",
              "      <td>0.254199</td>\n",
              "      <td>0.295172</td>\n",
              "      <td>47.763063</td>\n",
              "      <td>4.521419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>171.000000</td>\n",
              "      <td>61.000000</td>\n",
              "      <td>178.100000</td>\n",
              "      <td>453.549422</td>\n",
              "      <td>45207.000000</td>\n",
              "      <td>2.664300e+04</td>\n",
              "      <td>15.900000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>39.600000</td>\n",
              "      <td>...</td>\n",
              "      <td>48.453774</td>\n",
              "      <td>41.100000</td>\n",
              "      <td>36.300000</td>\n",
              "      <td>18.800000</td>\n",
              "      <td>90.059774</td>\n",
              "      <td>2.247576</td>\n",
              "      <td>0.549812</td>\n",
              "      <td>0.826185</td>\n",
              "      <td>51.669941</td>\n",
              "      <td>5.381478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>518.000000</td>\n",
              "      <td>149.000000</td>\n",
              "      <td>195.200000</td>\n",
              "      <td>480.850000</td>\n",
              "      <td>52492.000000</td>\n",
              "      <td>6.867100e+04</td>\n",
              "      <td>20.400000</td>\n",
              "      <td>83.650776</td>\n",
              "      <td>44.000000</td>\n",
              "      <td>42.500000</td>\n",
              "      <td>...</td>\n",
              "      <td>53.800000</td>\n",
              "      <td>47.700000</td>\n",
              "      <td>41.550000</td>\n",
              "      <td>23.100000</td>\n",
              "      <td>95.451693</td>\n",
              "      <td>10.509732</td>\n",
              "      <td>1.221037</td>\n",
              "      <td>2.177960</td>\n",
              "      <td>55.395132</td>\n",
              "      <td>6.493677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>38150.000000</td>\n",
              "      <td>14010.000000</td>\n",
              "      <td>362.800000</td>\n",
              "      <td>1206.900000</td>\n",
              "      <td>125635.000000</td>\n",
              "      <td>1.017029e+07</td>\n",
              "      <td>47.400000</td>\n",
              "      <td>9762.308998</td>\n",
              "      <td>624.000000</td>\n",
              "      <td>64.700000</td>\n",
              "      <td>...</td>\n",
              "      <td>78.900000</td>\n",
              "      <td>70.700000</td>\n",
              "      <td>65.100000</td>\n",
              "      <td>46.600000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>85.947799</td>\n",
              "      <td>42.619425</td>\n",
              "      <td>41.930251</td>\n",
              "      <td>78.075397</td>\n",
              "      <td>21.326165</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows  30 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        avganncount  avgdeathsperyear  target_deathrate  incidencerate  \\\n",
              "count   3047.000000       3047.000000       3047.000000    3047.000000   \n",
              "mean     606.338544        185.965868        178.664063     448.268586   \n",
              "std     1416.356223        504.134286         27.751511      54.560733   \n",
              "min        6.000000          3.000000         59.700000     201.300000   \n",
              "25%       76.000000         28.000000        161.200000     420.300000   \n",
              "50%      171.000000         61.000000        178.100000     453.549422   \n",
              "75%      518.000000        149.000000        195.200000     480.850000   \n",
              "max    38150.000000      14010.000000        362.800000    1206.900000   \n",
              "\n",
              "           medincome    popest2015  povertypercent  studypercap    medianage  \\\n",
              "count    3047.000000  3.047000e+03     3047.000000  3047.000000  3047.000000   \n",
              "mean    47063.281917  1.026374e+05       16.878175   155.399415    45.272333   \n",
              "std     12040.090836  3.290592e+05        6.409087   529.628366    45.304480   \n",
              "min     22640.000000  8.270000e+02        3.200000     0.000000    22.300000   \n",
              "25%     38882.500000  1.168400e+04       12.150000     0.000000    37.700000   \n",
              "50%     45207.000000  2.664300e+04       15.900000     0.000000    41.000000   \n",
              "75%     52492.000000  6.867100e+04       20.400000    83.650776    44.000000   \n",
              "max    125635.000000  1.017029e+07       47.400000  9762.308998   624.000000   \n",
              "\n",
              "       medianagemale  ...  pctprivatecoveragealone  pctempprivcoverage  \\\n",
              "count    3047.000000  ...              3047.000000         3047.000000   \n",
              "mean       39.570725  ...                48.453774           41.196324   \n",
              "std         5.226017  ...                 9.018885            9.447687   \n",
              "min        22.400000  ...                15.700000           13.500000   \n",
              "25%        36.350000  ...                43.100000           34.500000   \n",
              "50%        39.600000  ...                48.453774           41.100000   \n",
              "75%        42.500000  ...                53.800000           47.700000   \n",
              "max        64.700000  ...                78.900000           70.700000   \n",
              "\n",
              "       pctpubliccoverage  pctpubliccoveragealone     pctwhite     pctblack  \\\n",
              "count        3047.000000             3047.000000  3047.000000  3047.000000   \n",
              "mean           36.252642               19.240072    83.645286     9.107978   \n",
              "std             7.841741                6.113041    16.380025    14.534538   \n",
              "min            11.200000                2.600000    10.199155     0.000000   \n",
              "25%            30.900000               14.850000    77.296180     0.620675   \n",
              "50%            36.300000               18.800000    90.059774     2.247576   \n",
              "75%            41.550000               23.100000    95.451693    10.509732   \n",
              "max            65.100000               46.600000   100.000000    85.947799   \n",
              "\n",
              "          pctasian  pctotherrace  pctmarriedhouseholds    birthrate  \n",
              "count  3047.000000   3047.000000           3047.000000  3047.000000  \n",
              "mean      1.253965      1.983523             51.243872     5.640306  \n",
              "std       2.610276      3.517710              6.572814     1.985816  \n",
              "min       0.000000      0.000000             22.992490     0.000000  \n",
              "25%       0.254199      0.295172             47.763063     4.521419  \n",
              "50%       0.549812      0.826185             51.669941     5.381478  \n",
              "75%       1.221037      2.177960             55.395132     6.493677  \n",
              "max      42.619425     41.930251             78.075397    21.326165  \n",
              "\n",
              "[8 rows x 30 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "source": [
        "df=df.drop(columns='pctsomecol18_24')\r\n",
        "df['pctprivatecoveragealone'].fillna((df['pctprivatecoveragealone'].mean()), inplace=True)\r\n",
        "df['pctemployed16_over'].fillna((df['pctemployed16_over'].mean()), inplace=True)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "source": [
        "df.isna().any().sum()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "source": [
        "#Let me do it using the VIF function\r\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\r\n",
        "X=df.drop(columns=['target_deathrate', 'binnedinc','geography','pctmarriedhouseholds','medianagefemale','percentmarried'], axis=1)\r\n",
        "vif = pd.DataFrame()\r\n",
        "vif[\"features\"] = X.columns\r\n",
        "vif[\"vif_Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\r\n",
        "print(vif)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   features  vif_Factor\n",
            "0               avganncount   11.436189\n",
            "1          avgdeathsperyear   34.919935\n",
            "2             incidencerate   84.723027\n",
            "3                 medincome   99.683053\n",
            "4                popest2015   28.440313\n",
            "5            povertypercent   41.971202\n",
            "6               studypercap    1.134506\n",
            "7                 medianage    2.045323\n",
            "8             medianagemale  225.605578\n",
            "9              pctnohs18_24    9.284972\n",
            "10               pcths18_24   24.934868\n",
            "11          pctbachdeg18_24    5.386668\n",
            "12             pcths25_over   90.901776\n",
            "13        pctbachdeg25_over   37.470593\n",
            "14       pctemployed16_over  134.101416\n",
            "15     pctunemployed16_over   15.132445\n",
            "16       pctprivatecoverage  566.374755\n",
            "17  pctprivatecoveragealone  141.173399\n",
            "18       pctempprivcoverage  148.766122\n",
            "19        pctpubliccoverage  512.197091\n",
            "20   pctpubliccoveragealone  242.629236\n",
            "21                 pctwhite  156.546088\n",
            "22                 pctblack    6.648520\n",
            "23                 pctasian    2.405121\n",
            "24             pctotherrace    1.844506\n",
            "25                birthrate   10.483545\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "source": [
        "cols = list(vif['features'])\r\n",
        "cols"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['avganncount',\n",
              " 'avgdeathsperyear',\n",
              " 'incidencerate',\n",
              " 'medincome',\n",
              " 'popest2015',\n",
              " 'povertypercent',\n",
              " 'studypercap',\n",
              " 'medianage',\n",
              " 'medianagemale',\n",
              " 'pctnohs18_24',\n",
              " 'pcths18_24',\n",
              " 'pctbachdeg18_24',\n",
              " 'pcths25_over',\n",
              " 'pctbachdeg25_over',\n",
              " 'pctemployed16_over',\n",
              " 'pctunemployed16_over',\n",
              " 'pctprivatecoverage',\n",
              " 'pctprivatecoveragealone',\n",
              " 'pctempprivcoverage',\n",
              " 'pctpubliccoverage',\n",
              " 'pctpubliccoveragealone',\n",
              " 'pctwhite',\n",
              " 'pctblack',\n",
              " 'pctasian',\n",
              " 'pctotherrace',\n",
              " 'birthrate']"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "source": [
        "X = df[cols]\r\n",
        "y = df['target_deathrate']\r\n",
        "y"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       164.9\n",
              "1       161.3\n",
              "2       174.7\n",
              "3       194.8\n",
              "4       144.4\n",
              "        ...  \n",
              "3042    149.6\n",
              "3043    150.1\n",
              "3044    153.9\n",
              "3045    175.0\n",
              "3046    213.6\n",
              "Name: target_deathrate, Length: 3047, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "source": [
        "# Train using 80% of the data.\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\r\n",
        "\r\n",
        "# find optimal coefficients and intercept\r\n",
        "regressor = LinearRegression()  \r\n",
        "regressor.fit(X_train, y_train)\r\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "source": [
        "y_pred = regressor.predict(X_test)\r\n",
        "\r\n",
        "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \r\n",
        "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \r\n",
        "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error: 14.228185045844759\n",
            "Mean Squared Error: 350.79775416590314\n",
            "Root Mean Squared Error: 18.72959567545181\n"
          ]
        }
      ],
      "metadata": {}
    }
  ]
}